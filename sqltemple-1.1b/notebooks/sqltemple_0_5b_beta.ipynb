{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQLTemple-0.5B-Beta Training Pipeline\n",
    "\n",
    "This notebook implements the complete end-to-end training pipeline for SQLTemple-0.5B-Beta using knowledge distillation from TinyLlama-1.1B-Chat-v1.0.\n",
    "\n",
    "- **Teacher Model**: TinyLlama-1.1B-Chat-v1.0\n",
    "- **Student Model**: 0.5B parameter architecture\n",
    "- **Dataset**: Full Spider dataset (7,000 examples)\n",
    "- **Method**: Knowledge Distillation + LoRA fine-tuning\n",
    "- **Epochs**: 25 with optimizations\n",
    "- **Output**: GGUF-ready model for C++ runtime\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-cpp-python transformers datasets peft torch accelerate deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, AutoConfig,\n",
    "    TrainingArguments, Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "print(\"Starting SQLTemple-0.5B-Beta Training Pipeline\")\n",
    "print(f\"Started at: {datetime.now()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading\n",
    "\n",
    "Loading full Spider dataset for SQL training (7,000 examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Spider dataset...\")\n",
    "spider = load_dataset(\"xlangai/spider\", split=\"train\")\n",
    "\n",
    "print(f\"Spider: {len(spider):,} examples\")\n",
    "print(\"Sample Spider example:\")\n",
    "print(json.dumps(spider[0], indent=2)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer Setup\n",
    "\n",
    "Loading TinyLlama tokenizer and configuring for chat format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Tokenizer loaded\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"EOS token: '{tokenizer.eos_token}' (ID: {tokenizer.eos_token_id})\")\n",
    "print(f\"PAD token: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Converting Spider dataset into instruction format for chat-style training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_spider(example):\n",
    "    \"\"\"Convert Spider example to instruction format\"\"\"\n",
    "    question = example[\"question\"]\n",
    "    sql = example[\"query\"]\n",
    "    db_id = example.get(\"db_id\", \"\")\n",
    "\n",
    "    if db_id:\n",
    "        user_prompt = f\"Database: {db_id}\\nQuestion: {question}\"\n",
    "    else:\n",
    "        user_prompt = f\"Question: {question}\"\n",
    "\n",
    "    prompt = f\"<|system|>You are an SQL assistant. Answer in valid SQL.\\n<|user|>{user_prompt}\\n<|assistant|>\"\n",
    "    return {\"question\": question, \"sql\": sql, \"prompt\": prompt, \"db_id\": db_id}\n",
    "\n",
    "print(\"Testing preprocessing function...\")\n",
    "sample_spider = preprocess_spider(spider[0])\n",
    "\n",
    "print(\"Spider formatted example:\")\n",
    "print(f\"Prompt: {sample_spider['prompt'][:200]}...\")\n",
    "print(f\"SQL: {sample_spider['sql']}\")\n",
    "print(f\"DB ID: {sample_spider['db_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_example(example):\n",
    "    \"\"\"Preprocess a single Spider example for causal language modeling\"\"\"\n",
    "    processed = preprocess_spider(example)\n",
    "\n",
    "    prompt = processed[\"prompt\"]\n",
    "    sql = processed[\"sql\"]\n",
    "\n",
    "    full_text = prompt + sql + tokenizer.eos_token\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        full_text,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "    input_ids = tokenized[\"input_ids\"]\n",
    "    attention_mask = tokenized[\"attention_mask\"]\n",
    "\n",
    "    labels = input_ids.copy()\n",
    "\n",
    "    prompt_tokenized = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "    prompt_length = len([token for token in prompt_tokenized[\"input_ids\"] if token != tokenizer.pad_token_id])\n",
    "\n",
    "    for i in range(min(prompt_length, len(labels))):\n",
    "        labels[i] = -100\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        if attention_mask[i] == 0:\n",
    "            labels[i] = -100\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }\n",
    "\n",
    "print(\"Preprocessing full Spider dataset...\")\n",
    "\n",
    "tokenized_ds = spider.map(\n",
    "    preprocess_example,\n",
    "    remove_columns=spider.column_names,\n",
    "    desc=\"Processing Spider\"\n",
    ")\n",
    "\n",
    "print(f\"Processed dataset: {len(tokenized_ds):,} examples\")\n",
    "\n",
    "example = tokenized_ds[0]\n",
    "print(f\"All sequences are 512 tokens: {len(example['input_ids']) == 512}\")\n",
    "print(f\"Dataset features: {list(tokenized_ds.features.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teacher Model Loading\n",
    "\n",
    "Loading TinyLlama-1.1B as teacher model for knowledge distillation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading teacher model (TinyLlama-1.1B)...\")\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "teacher_params = sum(p.numel() for p in teacher_model.parameters())\n",
    "print(f\"Teacher model loaded: {teacher_params:,} parameters\")\n",
    "print(f\"Teacher model size: ~{teacher_params * 2 / 1e9:.2f} GB (bf16)\")\n",
    "\n",
    "# Set teacher model to eval mode\n",
    "teacher_model.eval()\n",
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"Teacher model set to evaluation mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Model Architecture\n",
    "\n",
    "Creating 0.5B parameter student model by scaling down TinyLlama architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Creating student model architecture...\")\n\nteacher_config = AutoConfig.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n\n# Create student config using the proper HuggingFace approach\n# Convert to dict, modify, then create new config\nconfig_dict = teacher_config.to_dict()\nconfig_dict.update({\n    \"num_hidden_layers\": 12,\n    \"hidden_size\": 1440,\n    \"intermediate_size\": 5760,\n    \"num_attention_heads\": 12,\n    \"num_key_value_heads\": 4\n})\n\nstudent_config = teacher_config.__class__(**config_dict)\n\nprint(f\"Student config:\")\nprint(f\"  Layers: {student_config.num_hidden_layers} (vs {teacher_config.num_hidden_layers})\")\nprint(f\"  Hidden size: {student_config.hidden_size} (vs {teacher_config.hidden_size})\")\nprint(f\"  Attention heads: {student_config.num_attention_heads} (vs {teacher_config.num_attention_heads})\")\n\nprint(\"Initializing student model...\")\nstudent_model = AutoModelForCausalLM.from_config(\n    student_config,\n    torch_dtype=torch.bfloat16\n)\n\nstudent_params = sum(p.numel() for p in student_model.parameters())\nprint(f\"Student model created: {student_params:,} parameters\")\nprint(f\"Student model size: ~{student_params * 2 / 1e9:.2f} GB (bf16)\")\nprint(f\"Size reduction: {(1 - student_params / teacher_params) * 100:.1f}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Configuration for Student\n",
    "\n",
    "Applying parameter-efficient fine-tuning with optimized LoRA adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Applying LoRA configuration to student model...\")\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "student_model = get_peft_model(student_model, lora_config)\n",
    "\n",
    "print(\"LoRA Parameter Summary:\")\n",
    "student_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Distillation Loss\n",
    "\n",
    "Custom trainer with knowledge distillation loss combining soft targets and feature alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class DistillationTrainer(Trainer):\n    def __init__(self, teacher_model, temperature=4.0, alpha=0.7, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.teacher_model = teacher_model\n        self.temperature = temperature\n        self.alpha = alpha\n        \n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        \"\"\"\n        Compute loss with knowledge distillation.\n        Updated to handle the num_items_in_batch parameter from newer Transformers versions.\n        \"\"\"\n        student_outputs = model(**inputs)\n        student_logits = student_outputs.logits\n        student_loss = student_outputs.loss\n        \n        with torch.no_grad():\n            teacher_outputs = self.teacher_model(**inputs)\n            teacher_logits = teacher_outputs.logits\n        \n        # Knowledge distillation loss\n        distillation_loss = F.kl_div(\n            F.log_softmax(student_logits / self.temperature, dim=-1),\n            F.softmax(teacher_logits / self.temperature, dim=-1),\n            reduction=\"batchmean\"\n        ) * (self.temperature ** 2)\n        \n        # Combined loss\n        loss = self.alpha * distillation_loss + (1 - self.alpha) * student_loss\n        \n        return (loss, student_outputs) if return_outputs else loss\n\nprint(\"Distillation trainer class defined\")\nprint(f\"Temperature: 4.0\")\nprint(f\"Alpha (distillation weight): 0.7\")\nprint(\"‚úÖ Updated to handle newer Transformers API\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration\n",
    "\n",
    "Setting up optimized training arguments for 25 epochs with full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Setting up training arguments...\")\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./sqltemple_0_5b_beta\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=25, \n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    warmup_steps=100,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    bf16=True,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=4,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured\")\n",
    "print(f\"Effective batch size: {args.per_device_train_batch_size * args.gradient_accumulation_steps}\")\n",
    "print(f\"Epochs: {args.num_train_epochs}\")\n",
    "print(f\"Learning rate: {args.learning_rate}\")\n",
    "print(f\"Scheduler: {args.lr_scheduler_type}\")\n",
    "print(f\"Mixed precision: {args.bf16}\")\n",
    "\n",
    "print(\"Preparing train/eval/test splits...\")\n",
    "dataset_size = len(tokenized_ds)\n",
    "train_size = int(dataset_size * 0.8)\n",
    "eval_size = int(dataset_size * 0.15)\n",
    "test_size = dataset_size - train_size - eval_size\n",
    "\n",
    "shuffled_ds = tokenized_ds.shuffle(seed=42)\n",
    "train_dataset = shuffled_ds.select(range(train_size))\n",
    "eval_dataset = shuffled_ds.select(range(train_size, train_size + eval_size))\n",
    "test_dataset = shuffled_ds.select(range(train_size + eval_size, dataset_size))\n",
    "\n",
    "print(f\"Training examples: {len(train_dataset):,}\")\n",
    "print(f\"Evaluation examples: {len(eval_dataset):,}\")\n",
    "print(f\"Test examples: {len(test_dataset):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Execute knowledge distillation training with optimized pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, EarlyStoppingCallback\n",
    "\n",
    "print(\"Setting up distillation training...\")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "trainer = DistillationTrainer(\n",
    "    teacher_model=teacher_model,\n",
    "    temperature=4.0,\n",
    "    alpha=0.7,\n",
    "    model=student_model,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")\n",
    "\n",
    "print(f\"Training started at: {datetime.now()}\")\n",
    "print(\"Training with knowledge distillation...\")\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(f\"Training completed at: {datetime.now()}\")\n",
    "print(f\"Training metrics: {train_result.metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Saving\n",
    "\n",
    "Saving the distilled student model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving model...\")\n",
    "\n",
    "print(\"Merging LoRA adapters...\")\n",
    "merged_model = student_model.merge_and_unload()\n",
    "\n",
    "print(\"Saving HuggingFace format...\")\n",
    "merged_model.save_pretrained(\n",
    "    \"./sqltemple-0.5b-beta-hf\",\n",
    "    safe_serialization=True,\n",
    "    push_to_hub=False\n",
    ")\n",
    "tokenizer.save_pretrained(\"./sqltemple-0.5b-beta-hf\")\n",
    "\n",
    "print(\"Model saved successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing\n",
    "\n",
    "Testing the distilled student model with sample SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing distilled model...\")\n",
    "merged_model.eval()\n",
    "\n",
    "test_prompts = [\n",
    "    \"<|system|>You are an SQL assistant. Answer in valid SQL.\\n<|user|>Question: Get all users from the users table\\n<|assistant|>\",\n",
    "    \"<|system|>You are an SQL assistant. Answer in valid SQL.\\n<|user|>Schema: products(id, name, price)\\nQuestion: Find products with price greater than 100\\n<|assistant|>\",\n",
    "    \"<|system|>You are an SQL assistant. Answer in valid SQL.\\n<|user|>Schema: employees(id, name, salary)\\nQuestion: List employees with salary less than 50000\\n<|assistant|>\"\n",
    "]\n",
    "\n",
    "print(\"Testing with sample prompts:\")\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n--- Test {i} ---\")\n",
    "    print(f\"Prompt: {prompt.split('<|assistant|>')[0]}\")\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = merged_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    sql_response = response.split(\"<|assistant|>\")[-1].strip()\n",
    "    print(f\"Generated SQL: {sql_response}\")\n",
    "\n",
    "print(\"Model testing completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Final summary of the knowledge distillation training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SQLTEMPLE-0.5B-BETA TRAINING COMPLETED\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"Teacher Model: TinyLlama-1.1B-Chat-v1.0\")\n",
    "print(f\"Student Model: ~0.5B parameters\")\n",
    "print(f\"Dataset: Spider (7,000 examples)\")\n",
    "print(f\"Training Examples: {len(train_dataset):,}\")\n",
    "print(f\"Method: Knowledge Distillation + LoRA\")\n",
    "print(f\"Epochs: {args.num_train_epochs}\")\n",
    "\n",
    "print(f\"\\nOptimizations Applied:\")\n",
    "print(f\"- Knowledge Distillation (T=4.0, Œ±=0.7)\")\n",
    "print(f\"- LoRA (r=8, Œ±=64)\")\n",
    "print(f\"- Mixed Precision Training (bf16)\")\n",
    "print(f\"- Cosine Learning Rate Schedule\")\n",
    "print(f\"- Gradient Clipping\")\n",
    "print(f\"- Early Stopping\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Card Generation\n",
    "\n",
    "Creating comprehensive model card with technical specifications and training details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate README.md for Hugging Face deployment\nreadme_content = f\"\"\"---\nlicense: apache-2.0\nbase_model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\ntags:\n- sql\n- text-to-sql\n- code-generation\n- knowledge-distillation\n- peft\n- lora\n- spider\n- database\n- query-generation\nlanguage:\n- en\ndatasets:\n- xlangai/spider\npipeline_tag: text-generation\nlibrary_name: transformers\nmodel_type: llama\nquantized: false\ninference:\n  parameters:\n    temperature: 0.7\n    max_new_tokens: 100\n    do_sample: true\nwidget:\n- example_title: \"Simple SELECT\"\n  text: \"<|system|>You are an SQL assistant. Answer in valid SQL.\\\\n<|user|>Question: Get all users from the users table\\\\n<|assistant|>\"\n- example_title: \"Conditional Query\"\n  text: \"<|system|>You are an SQL assistant. Answer in valid SQL.\\\\n<|user|>Schema: products(id, name, price)\\\\nQuestion: Find products with price greater than 100\\\\n<|assistant|>\"\n- example_title: \"Employee Query\"\n  text: \"<|system|>You are an SQL assistant. Answer in valid SQL.\\\\n<|user|>Schema: employees(id, name, salary)\\\\nQuestion: List employees with salary less than 50000\\\\n<|assistant|>\"\nmodel-index:\n- name: SQLTemple-0.5B-Beta\n  results:\n  - task:\n      type: text-generation\n      name: SQL Generation\n    dataset:\n      type: xlangai/spider\n      name: Spider\n      split: test\n    metrics:\n    - type: loss\n      name: Training Loss\n      value: \"TBD\"\n    - type: loss\n      name: Evaluation Loss\n      value: \"TBD\"\n---\n\n# SQLTemple-0.5B-Beta\n\n<div align=\"center\">\n\n![SQLTemple Logo](https://img.shields.io/badge/SQLTemple-0.5B--Beta-blue)\n![License](https://img.shields.io/badge/license-Apache%202.0-green)\n![Model Size](https://img.shields.io/badge/parameters-~{student_params//1000000}M-orange)\n![Base Model](https://img.shields.io/badge/base-TinyLlama--1.1B-purple)\n\n</div>\n\nSQLTemple-0.5B-Beta is a compact SQL generation model created through knowledge distillation from TinyLlama-1.1B-Chat-v1.0. This model is specifically designed for SQL query generation from natural language descriptions.\n\n## üöÄ Quick Start\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\n# Load model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"sqltemple/sqltemple-0.5b-beta\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"sqltemple/sqltemple-0.5b-beta\", \n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\"\n)\n\n# Generate SQL query\nprompt = \"<|system|>You are an SQL assistant. Answer in valid SQL.\\\\n<|user|>Question: Get all users from the users table\\\\n<|assistant|>\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=100,\n        temperature=0.7,\n        do_sample=True,\n        pad_token_id=tokenizer.eos_token_id\n    )\n\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nsql_query = response.split(\"<|assistant|>\")[-1].strip()\nprint(sql_query)  # Expected: SELECT * FROM users;\n```\n\n## üìã Model Details\n\n| **Attribute** | **Value** |\n|---------------|-----------|\n| **Model Name** | SQLTemple-0.5B-Beta |\n| **Model Type** | Causal Language Model (Transformer Decoder) |\n| **Base Model** | TinyLlama/TinyLlama-1.1B-Chat-v1.0 |\n| **Parameters** | ~{student_params:,} parameters |\n| **Size Reduction** | {(1 - student_params / teacher_params) * 100:.1f}% smaller than teacher model |\n| **License** | Apache 2.0 |\n| **Language** | SQL, English |\n| **Training Method** | Knowledge Distillation + LoRA |\n\n## üèóÔ∏è Architecture\n\n| **Component** | **Value** |\n|---------------|-----------|\n| **Layers** | {student_config.num_hidden_layers} |\n| **Hidden Size** | {student_config.hidden_size} |\n| **Attention Heads** | {student_config.num_attention_heads} |\n| **Key-Value Heads** | {student_config.num_key_value_heads} (Grouped Query Attention) |\n| **Intermediate Size** | {student_config.intermediate_size} |\n| **Vocabulary Size** | {student_config.vocab_size:,} |\n| **Max Position Embeddings** | {student_config.max_position_embeddings:,} |\n| **Activation Function** | SiLU |\n\n## üéØ Training\n\n### Method\nKnowledge Distillation + LoRA (Low-Rank Adaptation) Fine-tuning\n\n### Dataset\n- **Name**: Spider\n- **Source**: [xlangai/spider](https://huggingface.co/datasets/xlangai/spider)\n- **Total Examples**: {len(spider):,}\n- **Training Split**: {len(train_dataset):,} examples (80%)\n- **Evaluation Split**: {len(eval_dataset):,} examples (15%)\n- **Test Split**: {len(test_dataset):,} examples (5%)\n\n### Training Configuration\n| **Parameter** | **Value** |\n|---------------|-----------|\n| **Epochs** | {args.num_train_epochs} |\n| **Batch Size** | {args.per_device_train_batch_size} (per device) |\n| **Gradient Accumulation** | {args.gradient_accumulation_steps} steps |\n| **Effective Batch Size** | {args.per_device_train_batch_size * args.gradient_accumulation_steps} |\n| **Learning Rate** | {args.learning_rate} |\n| **Scheduler** | {args.lr_scheduler_type} |\n| **Precision** | bfloat16 |\n| **Max Sequence Length** | 512 tokens |\n\n### Knowledge Distillation\n| **Parameter** | **Value** |\n|---------------|-----------|\n| **Temperature** | 4.0 |\n| **Alpha (distillation weight)** | 0.7 |\n| **Loss Function** | KL Divergence + Cross Entropy |\n| **Soft Target Weight** | 70% |\n| **Hard Target Weight** | 30% |\n\n### LoRA Configuration\n| **Parameter** | **Value** |\n|---------------|-----------|\n| **Rank (r)** | {lora_config.r} |\n| **Alpha** | {lora_config.lora_alpha} |\n| **Target Modules** | {', '.join(lora_config.target_modules)} |\n| **Dropout** | {lora_config.lora_dropout} |\n| **Bias** | {lora_config.bias} |\n\n## üíª Usage\n\n### Input Format\nThe model uses a chat template with system, user, and assistant roles:\n\n```\n<|system|>You are an SQL assistant. Answer in valid SQL.\n<|user|>{{user_prompt}}\n<|assistant|>\n```\n\n### Example Queries\n\n#### Basic Query\n```python\nprompt = \"<|system|>You are an SQL assistant. Answer in valid SQL.\\\\n<|user|>Question: Show all customers\\\\n<|assistant|>\"\n# Expected output: SELECT * FROM customers;\n```\n\n#### Conditional Query\n```python\nprompt = \"<|system|>You are an SQL assistant. Answer in valid SQL.\\\\n<|user|>Schema: orders(id, customer_id, total)\\\\nQuestion: Find orders with total greater than 100\\\\n<|assistant|>\"\n# Expected output: SELECT * FROM orders WHERE total > 100;\n```\n\n#### Aggregation Query\n```python\nprompt = \"<|system|>You are an SQL assistant. Answer in valid SQL.\\\\n<|user|>Schema: sales(id, product_id, amount)\\\\nQuestion: What is the total sales amount?\\\\n<|assistant|>\"\n# Expected output: SELECT SUM(amount) FROM sales;\n```\n\n### Generation Parameters\n| **Parameter** | **Recommended Value** |\n|---------------|-----------------------|\n| **max_new_tokens** | 100 |\n| **temperature** | 0.7 |\n| **do_sample** | True |\n| **pad_token_id** | eos_token_id |\n\n## üéØ Intended Use\n\n- ‚úÖ **SQL query generation** from natural language\n- ‚úÖ **SQL code completion** and assistance\n- ‚úÖ **Educational SQL learning** tool\n- ‚úÖ **Integration into SQL IDEs** and development tools\n- ‚úÖ **Rapid prototyping** of database queries\n- ‚úÖ **SQL documentation** generation\n\n## ‚ö†Ô∏è Limitations\n\n- ‚ùå **Optimized primarily for SQL generation**, not general language tasks\n- ‚ùå **Training focused on Spider dataset patterns**\n- ‚ùå **May require fine-tuning** for domain-specific SQL dialects\n- ‚ùå **Limited to 512 token context window**\n- ‚ùå **No support for stored procedures** or advanced database features\n- ‚ùå **May not handle very complex queries** with multiple joins\n\n## üîß Performance\n\n### Hardware Requirements\n| **Component** | **Minimum** | **Recommended** |\n|---------------|-------------|-----------------|\n| **GPU Memory** | 8GB | 16GB+ |\n| **System RAM** | 16GB | 32GB+ |\n| **Inference Memory** | ~1GB (bfloat16) | ~2GB (float32) |\n| **Storage** | 1GB | 2GB |\n\n### Evaluation Metrics\n- **Training Loss**: TBD (logged during training)\n- **Evaluation Loss**: TBD (logged during training)\n- **Perplexity**: TBD (calculated post-training)\n- **Evaluation Strategy**: Steps-based evaluation every 500 steps\n- **Best Model Selection**: Lowest evaluation loss\n- **Early Stopping**: Patience of 5 evaluation steps\n\n## üîç Technical Details\n\n### Tokenizer\n- **Type**: LlamaTokenizer (from TinyLlama)\n- **EOS Token**: `</s>`\n- **PAD Token**: `</s>`\n- **Chat Tokens**: `<|system|>`, `<|user|>`, `<|assistant|>`\n\n### Data Preprocessing\n- **Instruction Format**: Chat-style with role markers\n- **Label Masking**: System and user prompts masked with -100\n- **Padding**: Right padding to max_length\n- **Truncation**: Enabled at 512 tokens\n\n### Model Outputs\n- üì¶ **HuggingFace format** (.safetensors)\n- üì¶ **GGUF format** (planned for C++ runtime)\n- üì¶ **LoRA adapters** (separate weights)\n\n## üîÑ Reproducibility\n\n| **Component** | **Value** |\n|---------------|-----------|\n| **Seed** | 42 |\n| **Framework** | PyTorch, Transformers, PEFT, Datasets |\n| **Training Script** | Jupyter notebook included |\n| **Data Shuffle** | Enabled with fixed seed |\n| **Hardware** | CUDA-compatible GPU |\n\n## üìö Citation\n\n```bibtex\n@misc{{sqltemple-0.5b-beta,\n  title={{SQLTemple-0.5B-Beta: Knowledge Distilled SQL Generation Model}},\n  author={{SQLTemple Development Team}},\n  year={{2024}},\n  note={{Model trained using knowledge distillation from TinyLlama-1.1B-Chat-v1.0}},\n  url={{https://huggingface.co/sqltemple/sqltemple-0.5b-beta}}\n}}\n```\n\n## üìÑ Model Card\n\nFor detailed technical specifications, training metrics, and additional information, see the complete model card at `model_card.json`.\n\n## ü§ù Contributing\n\nWe welcome contributions! Please see our [contributing guidelines](https://github.com/sqltemple/sqltemple) for more information.\n\n## üìû Support\n\n- **GitHub Issues**: [Report bugs and feature requests](https://github.com/sqltemple/sqltemple/issues)\n- **Discussions**: [Community discussions](https://github.com/sqltemple/sqltemple/discussions)\n- **Documentation**: [Full documentation](https://sqltemple.github.io/docs)\n\n---\n\n<div align=\"center\">\n\n**Created by**: SQLTemple Project  \n**Date**: {datetime.now().strftime('%Y-%m-%d')}  \n**Version**: 1.0\n\n[![GitHub](https://img.shields.io/badge/GitHub-sqltemple-blue)](https://github.com/sqltemple/sqltemple)\n[![Hugging Face](https://img.shields.io/badge/ü§ó-Hugging%20Face-yellow)](https://huggingface.co/sqltemple)\n\n</div>\n\"\"\"\n\n# Save README.md with proper HF frontmatter\nreadme_path = \"./sqltemple-0.5b-beta-hf/README.md\"\nwith open(readme_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(readme_content)\n\nprint(f\"‚úÖ Hugging Face README.md saved to: {readme_path}\")\nprint(f\"üìä README.md length: {len(readme_content):,} characters\")\nprint(\"\\\\nüöÄ Hugging Face Deployment Features Added:\")\nprint(\"  ‚úÖ YAML frontmatter with model metadata\")\nprint(\"  ‚úÖ License, tags, and pipeline information\")\nprint(\"  ‚úÖ Widget examples for HF interface\")\nprint(\"  ‚úÖ Model index with evaluation metrics\")\nprint(\"  ‚úÖ Proper formatting with badges and tables\")\nprint(\"  ‚úÖ HF-specific sections and links\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}